{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f55c8fdd-a67b-4ae5-b348-46ff1652ab83",
   "metadata": {},
   "source": [
    "# 自然语言推断与数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a2396acb-4705-4f21-885f-6b0d1284ffff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File exists: True\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import torch\n",
    "from torch import nn\n",
    "from d2l import torch as d2l\n",
    "\n",
    "#d2l.DATA_HUB['SNLI'] = (\n",
    "#    'https://nlp.stanford.edu/projects/snli/snli_1.0.zip',\n",
    "#    '9fcde07509c7e87ec61c640c1b2753d9041758e4')\n",
    "\n",
    "data_dir = '..\\\\data\\\\snli_1.0'  # 替换为你的实际路径\n",
    "file_name = os.path.join(data_dir, 'snli_1.0_train.txt')\n",
    "print(\"File exists:\", os.path.exists(file_name))  # 检查文件是否存在"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e17bb674-12d5-4762-a2d6-ca6bb3f538cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_snli(data_dir, is_train):\n",
    "    \"\"\"将SNLI数据集解析为前提、假设和标签\"\"\"\n",
    "    def extract_text(s):\n",
    "        # 删除我们不会使用的信息\n",
    "        s = re.sub('\\\\(', '', s)\n",
    "        s = re.sub('\\\\)', '', s)\n",
    "        # 用一个空格替换两个或多个连续的空格\n",
    "        s = re.sub('\\\\s{2,}', ' ', s)\n",
    "        return s.strip()\n",
    "    label_set = {'entailment': 0, 'contradiction': 1, 'neutral': 2}\n",
    "    file_name = os.path.join(data_dir, 'snli_1.0_train.txt'\n",
    "                             if is_train else 'snli_1.0_test.txt')\n",
    "    with open(file_name, 'r') as f:\n",
    "        rows = [row.split('\\t') for row in f.readlines()[1:]]\n",
    "    premises = [extract_text(row[1]) for row in rows if row[0] in label_set]\n",
    "    hypotheses = [extract_text(row[2]) for row in rows if row[0] \\\n",
    "                in label_set]\n",
    "    labels = [label_set[row[0]] for row in rows if row[0] in label_set]\n",
    "    return premises, hypotheses, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ae39d1a4-80fc-4b53-84a8-afcf58fc4ed6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "前提： A person on a horse jumps over a broken down airplane .\n",
      "假设： A person is training his horse for a competition .\n",
      "标签： 2\n",
      "前提： A person on a horse jumps over a broken down airplane .\n",
      "假设： A person is at a diner , ordering an omelette .\n",
      "标签： 1\n",
      "前提： A person on a horse jumps over a broken down airplane .\n",
      "假设： A person is outdoors , on a horse .\n",
      "标签： 0\n"
     ]
    }
   ],
   "source": [
    "train_data = read_snli(data_dir, is_train=True)\n",
    "for x0, x1, y in zip(train_data[0][:3], train_data[1][:3], train_data[2][:3]):\n",
    "    print('前提：', x0)\n",
    "    print('假设：', x1)\n",
    "    print('标签：', y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fd296ef9-319d-4904-bf0d-16e926666fcc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[183416, 183187, 182764]\n",
      "[3368, 3237, 3219]\n"
     ]
    }
   ],
   "source": [
    "test_data = read_snli(data_dir, is_train=False)\n",
    "for data in [train_data, test_data]:\n",
    "    print([[row for row in data[2]].count(i) for i in range(3)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2c7c5916-6574-4dd0-87cb-567d690fcb93",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SNLIDataset(torch.utils.data.Dataset):\n",
    "    \"\"\"用于加载SNLI数据集的自定义数据集\"\"\"\n",
    "    def __init__(self, dataset, num_steps, vocab=None):\n",
    "        self.num_steps = num_steps\n",
    "        all_premise_tokens = d2l.tokenize(dataset[0])\n",
    "        all_hypothesis_tokens = d2l.tokenize(dataset[1])\n",
    "        if vocab is None:\n",
    "            self.vocab = d2l.Vocab(all_premise_tokens + \\\n",
    "                all_hypothesis_tokens, min_freq=5, reserved_tokens=['<pad>'])\n",
    "        else:\n",
    "            self.vocab = vocab\n",
    "        self.premises = self._pad(all_premise_tokens)\n",
    "        self.hypotheses = self._pad(all_hypothesis_tokens)\n",
    "        self.labels = torch.tensor(dataset[2])\n",
    "        print('read ' + str(len(self.premises)) + ' examples')\n",
    "\n",
    "    def _pad(self, lines):\n",
    "        return torch.tensor([d2l.truncate_pad(\n",
    "            self.vocab[line], self.num_steps, self.vocab['<pad>'])\n",
    "                         for line in lines])\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return (self.premises[idx], self.hypotheses[idx]), self.labels[idx]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.premises)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "81fabbb0-d0bf-4b06-af57-178c8ac2cc0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data_snli(batch_size, num_steps=50):\n",
    "    \"\"\"下载SNLI数据集并返回数据迭代器和词表\"\"\"\n",
    "    num_workers = d2l.get_dataloader_workers()\n",
    "    data_dir = '..\\\\data\\\\snli_1.0'\n",
    "    train_data = read_snli(data_dir, True)\n",
    "    test_data = read_snli(data_dir, False)\n",
    "    train_set = SNLIDataset(train_data, num_steps)\n",
    "    test_set = SNLIDataset(test_data, num_steps, train_set.vocab)\n",
    "    train_iter = torch.utils.data.DataLoader(train_set, batch_size,\n",
    "                                             shuffle=True,\n",
    "                                             num_workers=num_workers)\n",
    "    test_iter = torch.utils.data.DataLoader(test_set, batch_size,\n",
    "                                            shuffle=False,\n",
    "                                            num_workers=num_workers)\n",
    "    return train_iter, test_iter, train_set.vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4b515511-99fc-4882-9fd8-29d9f1af6654",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "read 549367 examples\n",
      "read 9824 examples\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "18678"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_iter, test_iter, vocab = load_data_snli(128, 50)\n",
    "len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1bf5dfa-a51a-45f9-9247-a352c43f0614",
   "metadata": {},
   "outputs": [],
   "source": [
    "for X, Y in train_iter:\n",
    "    print(X[0].shape)\n",
    "    print(X[1].shape)\n",
    "    print(Y.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48d6af14-2a6e-4a80-b45b-adca3e154121",
   "metadata": {},
   "source": [
    "# 针对序列级和词元级应用微调BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f02ea42c-d20f-4b46-9689-2773a8108588",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import multiprocessing\n",
    "import os\n",
    "import torch\n",
    "from torch import nn\n",
    "from d2l import torch as d2l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8d6513b0-a513-401f-8b86-73c916277764",
   "metadata": {},
   "outputs": [],
   "source": [
    "d2l.DATA_HUB['bert.base'] = (d2l.DATA_URL + 'bert.base.torch.zip',\n",
    "                             '225d66f04cae318b841a13d32af3acc165f253ac')\n",
    "d2l.DATA_HUB['bert.small'] = (d2l.DATA_URL + 'bert.small.torch.zip',\n",
    "                              'c72329e68a732bef0452e4b96a1c341c8910f81f')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "9ffb71a0-a161-434a-8740-479398dcb679",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import d2l\n",
    "\n",
    "def get_tokens_and_segments(tokens_a, tokens_b=None):\n",
    "    \"\"\"\n",
    "    处理输入序列的词元并生成片段索引。\n",
    "    \n",
    "    参数:\n",
    "    tokens_a (list): 第一个句子的词元列表。\n",
    "    tokens_b (list, optional): 第二个句子的词元列表（可选，用于句子对任务）。\n",
    "    \n",
    "    返回:\n",
    "    tuple: 包含两个元素：\n",
    "        - tokens (list): 处理后的词元序列，包含特殊标记<cls>和<SEP>。\n",
    "        - segments (list): 片段索引列表，0表示属于第一个句子，1表示属于第二个句子。\n",
    "    \"\"\"\n",
    "    # 添加句子开始标记<cls>和第一个句子结束标记<SEP>\n",
    "    tokens = ['<cls>'] + tokens_a + ['<sep>']\n",
    "    # 初始化片段索引，0表示第一个句子及其前后的特殊标记\n",
    "    segments = [0] * (len(tokens_a) + 2)\n",
    "    # 如果存在第二个句子，则添加<SEP>和第二个句子的词元，并标记片段索引为1\n",
    "    if tokens_b is not None:\n",
    "        tokens += tokens_b + ['<sep>']\n",
    "        segments += [1] * (len(tokens_b) + 1)\n",
    "    return tokens, segments\n",
    "\n",
    "class PositionWiseFFN(nn.Module):\n",
    "    \"\"\"\n",
    "    逐位置前馈神经网络（Position-wise Feed-Forward Network）。\n",
    "    对序列中的每个位置独立应用相同的全连接神经网络，用于特征变换。\n",
    "    \"\"\"\n",
    "    def __init__(self, ffn_num_input, ffn_num_hiddens, ffn_num_outputs,\n",
    "                 **kwargs):\n",
    "        \"\"\"\n",
    "        初始化逐位置前馈网络。\n",
    "        \n",
    "        参数:\n",
    "        ffn_num_input (int): 输入特征维度。\n",
    "        ffn_num_hiddens (int): 隐藏层维度。\n",
    "        ffn_num_outputs (int): 输出特征维度。\n",
    "        **kwargs: 其他关键字参数，传递给父类。\n",
    "        \"\"\"\n",
    "        super(PositionWiseFFN, self).__init__(**kwargs)\n",
    "        # 第一个全连接层，将输入映射到隐藏层\n",
    "        self.dense1 = nn.Linear(ffn_num_input, ffn_num_hiddens)\n",
    "        # ReLU激活函数，引入非线性变换\n",
    "        self.relu = nn.ReLU()\n",
    "        # 第二个全连接层，将隐藏层映射到输出\n",
    "        self.dense2 = nn.Linear(ffn_num_hiddens, ffn_num_outputs)\n",
    "\n",
    "    def forward(self, X):\n",
    "        \"\"\"\n",
    "        前向传播。\n",
    "        \n",
    "        参数:\n",
    "        X (torch.Tensor): 输入张量，形状为(batch_size, seq_len, ffn_num_input)。\n",
    "        \n",
    "        返回:\n",
    "        torch.Tensor: 输出张量，形状为(batch_size, seq_len, ffn_num_outputs)。\n",
    "        \"\"\"\n",
    "        return self.dense2(self.relu(self.dense1(X)))\n",
    "\n",
    "class AddNorm(nn.Module):\n",
    "    \"\"\"\n",
    "    残差连接与层归一化（Add & Norm）模块。\n",
    "    用于将输入与子层的输出相加，并应用层归一化。\n",
    "    \"\"\"\n",
    "    def __init__(self, normalized_shape, dropout, **kwargs):\n",
    "        \"\"\"\n",
    "        初始化AddNorm模块。\n",
    "        \n",
    "        参数:\n",
    "        normalized_shape (tuple): 层归一化的形状。\n",
    "        dropout (float): Dropout概率，用于防止过拟合。\n",
    "        **kwargs: 其他关键字参数，传递给父类。\n",
    "        \"\"\"\n",
    "        super(AddNorm, self).__init__(**kwargs)\n",
    "        # Dropout层，用于在子层输出中随机丢弃部分神经元\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        # 层归一化层，对最后一个维度进行归一化\n",
    "        self.ln = nn.LayerNorm(normalized_shape)\n",
    "\n",
    "    def forward(self, X, Y):\n",
    "        \"\"\"\n",
    "        前向传播。\n",
    "        \n",
    "        参数:\n",
    "        X (torch.Tensor): 输入张量。\n",
    "        Y (torch.Tensor): 子层的输出张量。\n",
    "        \n",
    "        返回:\n",
    "        torch.Tensor: 应用残差连接和层归一化后的输出。\n",
    "        \"\"\"\n",
    "        return self.ln(self.dropout(Y) + X)\n",
    "\n",
    "class EncoderBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    BERT编码器块，包含多头注意力和逐位置前馈网络。\n",
    "    \"\"\"\n",
    "    def __init__(self, key_size, query_size, value_size, num_hiddens,\n",
    "                 norm_shape, ffn_num_input, ffn_num_hiddens, num_heads,\n",
    "                 dropout, use_bias=False, **kwargs):\n",
    "        \"\"\"\n",
    "        初始化编码器块。\n",
    "        \n",
    "        参数:\n",
    "        key_size (int): 键的特征维度。\n",
    "        query_size (int): 查询的特征维度。\n",
    "        value_size (int): 值的特征维度。\n",
    "        num_hiddens (int): 隐藏层维度。\n",
    "        norm_shape (tuple): 层归一化的形状。\n",
    "        ffn_num_input (int): 前馈网络的输入维度。\n",
    "        ffn_num_hiddens (int): 前馈网络的隐藏层维度。\n",
    "        num_heads (int): 多头注意力的头数。\n",
    "        dropout (float): Dropout概率。\n",
    "        use_bias (bool): 是否在多头注意力中使用偏置。\n",
    "        **kwargs: 其他关键字参数，传递给父类。\n",
    "        \"\"\"\n",
    "        super(EncoderBlock, self).__init__(**kwargs)\n",
    "        # 多头注意力层，用于捕捉序列中的上下文信息\n",
    "        self.attention = d2l.MultiHeadAttention(\n",
    "            num_hiddens=num_hiddens,\n",
    "            num_heads=num_heads,\n",
    "            dropout=dropout,\n",
    "            use_bias=use_bias\n",
    "        )\n",
    "        # 第一个AddNorm模块，用于多头注意力后的残差连接和层归一化\n",
    "        self.addnorm1 = AddNorm(norm_shape, dropout)\n",
    "        # 逐位置前馈网络，用于进一步特征变换\n",
    "        self.ffn = PositionWiseFFN(ffn_num_input, ffn_num_hiddens, num_hiddens)\n",
    "        # 第二个AddNorm模块，用于前馈网络后的残差连接和层归一化\n",
    "        self.addnorm2 = AddNorm(norm_shape, dropout)\n",
    "\n",
    "    def forward(self, X, valid_lens):\n",
    "        \"\"\"\n",
    "        前向传播。\n",
    "        \n",
    "        参数:\n",
    "        X (torch.Tensor): 输入张量，形状为(batch_size, seq_len, num_hiddens)。\n",
    "        valid_lens (torch.Tensor): 有效长度张量，用于遮蔽填充部分。\n",
    "        \n",
    "        返回:\n",
    "        torch.Tensor: 编码器块的输出张量。\n",
    "        \"\"\"\n",
    "        # 多头注意力计算，并通过AddNorm处理残差连接和层归一化\n",
    "        Y = self.addnorm1(X, self.attention(X, X, X, valid_lens))\n",
    "        # 前馈网络计算，并通过AddNorm处理残差连接和层归一化\n",
    "        return self.addnorm2(Y, self.ffn(Y))\n",
    "\n",
    "class BERTEncoder(nn.Module):\n",
    "    \"\"\"\n",
    "    BERT编码器，包含嵌入层、位置编码和多个编码器块。\n",
    "    \"\"\"\n",
    "    def __init__(self, vocab_size, num_hiddens, norm_shape, ffn_num_input,\n",
    "                 ffn_num_hiddens, num_heads, num_layers, dropout,\n",
    "                 max_len=1000, key_size=768, query_size=768, value_size=768,\n",
    "                 **kwargs):\n",
    "        \"\"\"\n",
    "        初始化BERT编码器。\n",
    "        \n",
    "        参数:\n",
    "        vocab_size (int): 词汇表大小。\n",
    "        num_hiddens (int): 隐藏层维度。\n",
    "        norm_shape (tuple): 层归一化的形状。\n",
    "        ffn_num_input (int): 前馈网络的输入维度。\n",
    "        ffn_num_hiddens (int): 前馈网络的隐藏层维度。\n",
    "        num_heads (int): 多头注意力的头数。\n",
    "        num_layers (int): 编码器块的数量。\n",
    "        dropout (float): Dropout概率。\n",
    "        max_len (int): 最大序列长度，用于位置编码。\n",
    "        key_size (int): 键的特征维度。\n",
    "        query_size (int): 查询的特征维度。\n",
    "        value_size (int): 值的特征维度。\n",
    "        **kwargs: 其他关键字参数，传递给父类。\n",
    "        \"\"\"\n",
    "        super(BERTEncoder, self).__init__(**kwargs)\n",
    "        # 词元嵌入层，将词元索引转换为词向量\n",
    "        self.token_embedding = nn.Embedding(vocab_size, num_hiddens)\n",
    "        # 片段嵌入层，标记句子A和句子B（0或1）\n",
    "        self.segment_embedding = nn.Embedding(2, num_hiddens)\n",
    "        # 按顺序添加多个编码器块\n",
    "        self.blks = nn.Sequential()\n",
    "        for i in range(num_layers):\n",
    "            self.blks.add_module(f\"{i}\", EncoderBlock(\n",
    "                key_size, query_size, value_size, num_hiddens, norm_shape,\n",
    "                ffn_num_input, ffn_num_hiddens, num_heads, dropout, True))\n",
    "        # 可学习的位置嵌入参数，用于捕捉序列的位置信息\n",
    "        self.pos_embedding = nn.Parameter(torch.randn(1, max_len, num_hiddens))\n",
    "\n",
    "    def forward(self, tokens, segments, valid_lens):\n",
    "        \"\"\"\n",
    "        前向传播。\n",
    "        \n",
    "        参数:\n",
    "        tokens (torch.Tensor): 词元索引张量，形状为(batch_size, seq_len)。\n",
    "        segments (torch.Tensor): 片段索引张量，形状为(batch_size, seq_len)。\n",
    "        valid_lens (torch.Tensor): 有效长度张量，形状为(batch_size,)。\n",
    "        \n",
    "        返回:\n",
    "        torch.Tensor: 编码器的输出张量，形状为(batch_size, seq_len, num_hiddens)。\n",
    "        \"\"\"\n",
    "        # 词元嵌入、片段嵌入和位置嵌入相加\n",
    "        X = self.token_embedding(tokens) + self.segment_embedding(segments)\n",
    "        X = X + self.pos_embedding.data[:, :X.shape[1], :]\n",
    "        # 依次通过每个编码器块\n",
    "        for blk in self.blks:\n",
    "            X = blk(X, valid_lens)\n",
    "        return X\n",
    "\n",
    "class MaskLM(nn.Module):\n",
    "    \"\"\"\n",
    "    掩蔽语言模型（Masked Language Model）任务模块。\n",
    "    用于预测输入中被遮蔽的词元。\n",
    "    \"\"\"\n",
    "    def __init__(self, vocab_size, num_hiddens, num_inputs=768, **kwargs):\n",
    "        \"\"\"\n",
    "        初始化MaskLM模块。\n",
    "        \n",
    "        参数:\n",
    "        vocab_size (int): 词汇表大小。\n",
    "        num_hiddens (int): 隐藏层维度。\n",
    "        num_inputs (int): 输入特征维度。\n",
    "        **kwargs: 其他关键字参数，传递给父类。\n",
    "        \"\"\"\n",
    "        super(MaskLM, self).__init__(**kwargs)\n",
    "        # 多层感知机（MLP），用于将输入特征映射到词汇表维度\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(num_inputs, num_hiddens),\n",
    "            nn.ReLU(),\n",
    "            nn.LayerNorm(num_hiddens),\n",
    "            nn.Linear(num_hiddens, vocab_size)\n",
    "        )\n",
    "\n",
    "    def forward(self, X, pred_positions):\n",
    "        \"\"\"\n",
    "        前向传播。\n",
    "        \n",
    "        参数:\n",
    "        X (torch.Tensor): 编码器的输出张量，形状为(batch_size, seq_len, num_inputs)。\n",
    "        pred_positions (torch.Tensor): 被遮蔽位置的张量，形状为(batch_size, num_preds)。\n",
    "        \n",
    "        返回:\n",
    "        torch.Tensor: 预测的词元概率分布，形状为(batch_size, num_preds, vocab_size)。\n",
    "        \"\"\"\n",
    "        # 获取预测位置的数量和批次大小\n",
    "        num_pred_positions = pred_positions.shape[1]\n",
    "        pred_positions = pred_positions.reshape(-1)\n",
    "        batch_size = X.shape[0]\n",
    "        # 生成批次索引，用于提取遮蔽位置的特征\n",
    "        batch_idx = torch.arange(0, batch_size)\n",
    "        batch_idx = torch.repeat_interleave(batch_idx, num_pred_positions)\n",
    "        # 提取遮蔽位置的特征\n",
    "        masked_X = X[batch_idx, pred_positions]\n",
    "        masked_X = masked_X.reshape((batch_size, num_pred_positions, -1))\n",
    "        # 通过MLP预测被遮蔽的词元\n",
    "        mlm_Y_hat = self.mlp(masked_X)\n",
    "        return mlm_Y_hat\n",
    "\n",
    "class NextSentencePred(nn.Module):\n",
    "    \"\"\"\n",
    "    下一句预测（Next Sentence Prediction）任务模块。\n",
    "    用于判断两个句子是否为连续的上下文。\n",
    "    \"\"\"\n",
    "    def __init__(self, num_inputs, **kwargs):\n",
    "        \"\"\"\n",
    "        初始化NextSentencePred模块。\n",
    "        \n",
    "        参数:\n",
    "        num_inputs (int): 输入特征维度（通常为BERT编码器输出的隐藏层维度）。\n",
    "        **kwargs: 其他关键字参数，传递给父类。\n",
    "        \"\"\"\n",
    "        super(NextSentencePred, self).__init__(**kwargs)\n",
    "        # 全连接层，将输入特征映射到两个类别（是/否为下一句）\n",
    "        self.output = nn.Linear(num_inputs, 2)\n",
    "\n",
    "    def forward(self, X):\n",
    "        \"\"\"\n",
    "        前向传播。\n",
    "        \n",
    "        参数:\n",
    "        X (torch.Tensor): 输入张量，形状为(batch_size, num_inputs)。\n",
    "        \n",
    "        返回:\n",
    "        torch.Tensor: 预测的概率分布，形状为(batch_size, 2)。\n",
    "        \"\"\"\n",
    "        return self.output(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ab925d01-66b8-4653-a520-3ca297a21348",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class BERTModel(nn.Module):\n",
    "    \"\"\"\n",
    "    BERT模型，整合了编码器、掩蔽语言模型（MLM）和下一句预测（NSP）任务。\n",
    "    \n",
    "    BERT通过无监督预训练学习通用的文本表示，可通过微调适应多种下游任务。\n",
    "    \"\"\"\n",
    "    def __init__(self, vocab_size, num_hiddens, norm_shape, ffn_num_input,\n",
    "                 ffn_num_hiddens, num_heads, num_layers, dropout,\n",
    "                 max_len=1000, key_size=768, query_size=768, value_size=768,\n",
    "                 hid_in_features=768, mlm_in_features=768,\n",
    "                 nsp_in_features=768):\n",
    "        \"\"\"\n",
    "        初始化BERT模型。\n",
    "        \n",
    "        参数:\n",
    "        vocab_size (int): 词汇表大小，决定词元嵌入的维度。\n",
    "        num_hiddens (int): 编码器的隐藏层维度，即特征向量的维度。\n",
    "        norm_shape (tuple): 层归一化的形状，通常为(num_hiddens,)。\n",
    "        ffn_num_input (int): 前馈网络的输入维度。\n",
    "        ffn_num_hiddens (int): 前馈网络的隐藏层维度。\n",
    "        num_heads (int): 多头注意力的头数，用于并行捕捉不同子空间的信息。\n",
    "        num_layers (int): 编码器块的数量，决定模型的深度。\n",
    "        dropout (float): Dropout概率，用于防止过拟合。\n",
    "        max_len (int): 输入序列的最大长度，用于位置嵌入的初始化。\n",
    "        key_size (int): 多头注意力中键的特征维度。\n",
    "        query_size (int): 多头注意力中查询的特征维度。\n",
    "        value_size (int): 多头注意力中值的特征维度。\n",
    "        hid_in_features (int): 下一句预测任务隐藏层的输入维度。\n",
    "        mlm_in_features (int): 掩蔽语言模型任务的输入特征维度。\n",
    "        nsp_in_features (int): 下一句预测任务的输入特征维度。\n",
    "        \"\"\"\n",
    "        super(BERTModel, self).__init__()\n",
    "        # 初始化BERT编码器，处理词元、片段和位置嵌入，并通过多层编码器块提取上下文特征\n",
    "        self.encoder = BERTEncoder(vocab_size, num_hiddens, norm_shape,\n",
    "                    ffn_num_input, ffn_num_hiddens, num_heads, num_layers,\n",
    "                    dropout, max_len=max_len, key_size=key_size,\n",
    "                    query_size=query_size, value_size=value_size)\n",
    "        # 下一句预测任务的隐藏层，将编码器输出的CLS标记映射到num_hiddens维度\n",
    "        self.hidden = nn.Sequential(\n",
    "            nn.Linear(hid_in_features, num_hiddens),  # 线性变换\n",
    "            nn.Tanh()  # Tanh激活函数引入非线性\n",
    "        )\n",
    "        # 掩蔽语言模型（MLM）模块，用于预测输入中被遮蔽的词元\n",
    "        self.mlm = MaskLM(vocab_size, num_hiddens, mlm_in_features)\n",
    "        # 下一句预测（NSP）模块，用于判断两个句子是否为连续的上下文\n",
    "        self.nsp = NextSentencePred(nsp_in_features)\n",
    "\n",
    "    def forward(self, tokens, segments, valid_lens=None,\n",
    "                pred_positions=None):\n",
    "        \"\"\"\n",
    "        BERT模型的前向传播。\n",
    "        \n",
    "        参数:\n",
    "        tokens (torch.Tensor): 词元索引张量，形状为(batch_size, seq_len)。\n",
    "        segments (torch.Tensor): 片段索引张量，0和1分别标记句子A和B，形状同上。\n",
    "        valid_lens (torch.Tensor): 有效长度张量，用于遮蔽填充部分，形状为(batch_size,)。\n",
    "        pred_positions (torch.Tensor): 被遮蔽词元的位置张量，形状为(batch_size, num_preds)。\n",
    "        \n",
    "        返回:\n",
    "        tuple: 包含三个元素：\n",
    "            - encoded_X (torch.Tensor): 编码器的输出，形状为(batch_size, seq_len, num_hiddens)。\n",
    "            - mlm_Y_hat (torch.Tensor): MLM任务的预测结果，形状为(batch_size, num_preds, vocab_size)。\n",
    "            - nsp_Y_hat (torch.Tensor): NSP任务的预测结果，形状为(batch_size, 2)。\n",
    "        \"\"\"\n",
    "        # 通过BERT编码器获取上下文表示\n",
    "        encoded_X = self.encoder(tokens, segments, valid_lens)\n",
    "        # 如果提供了被遮蔽位置，则计算MLM任务的预测结果\n",
    "        if pred_positions is not None:\n",
    "            mlm_Y_hat = self.mlm(encoded_X, pred_positions)\n",
    "        else:\n",
    "            mlm_Y_hat = None\n",
    "        # 提取CLS标记的特征（序列的第一个位置），用于下一句预测任务\n",
    "        # 先通过隐藏层进行变换，再输入到NSP模块\n",
    "        nsp_Y_hat = self.nsp(self.hidden(encoded_X[:, 0, :]))\n",
    "        return encoded_X, mlm_Y_hat, nsp_Y_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "75068230-c3b2-44e1-9295-6aa5c1eb969e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_pretrained_model(pretrained_model, num_hiddens, ffn_num_hiddens,\n",
    "                          num_heads, num_layers, dropout, max_len, devices):\n",
    "    \"\"\"\n",
    "    加载预训练的BERT模型，并适配当前配置。\n",
    "    \n",
    "    参数:\n",
    "    pretrained_model (str): 预训练模型的名称或路径。\n",
    "    num_hiddens (int): 模型隐藏层维度。\n",
    "    ffn_num_hiddens (int): 前馈网络隐藏层维度。\n",
    "    num_heads (int): 多头注意力的头数。\n",
    "    num_layers (int): 编码器层数。\n",
    "    dropout (float): Dropout概率。\n",
    "    max_len (int): 输入序列的最大长度。\n",
    "    devices (list): 可用的计算设备（如GPU列表）。\n",
    "    \n",
    "    返回:\n",
    "    tuple: 包含加载了预训练参数的BERT模型和对应的词汇表。\n",
    "    \"\"\"\n",
    "    # 下载并解压预训练模型的数据（假设d2l.download_extract已实现）\n",
    "    data_dir = d2l.download_extract(pretrained_model)\n",
    "    \n",
    "    # 加载词汇表：从预训练模型的vocab.json文件中读取词表\n",
    "    vocab = d2l.Vocab()\n",
    "    # 读取JSON文件，将索引到词的映射加载到vocab.idx_to_token\n",
    "    vocab.idx_to_token = json.load(open(os.path.join(data_dir, 'vocab.json')))\n",
    "    # 构建词到索引的反向映射\n",
    "    vocab.token_to_idx = {token: idx for idx, token in enumerate(vocab.idx_to_token)}\n",
    "    \n",
    "    # 初始化BERT模型，确保参数与预训练模型结构匹配\n",
    "    bert = BERTModel(\n",
    "        len(vocab),               # 词汇表大小，决定词元嵌入维度\n",
    "        num_hiddens=num_hiddens,  # 隐藏层维度\n",
    "        norm_shape=[256],         # 层归一化的形状，需与预训练模型一致\n",
    "        ffn_num_input=256,        # 前馈网络输入维度\n",
    "        ffn_num_hiddens=ffn_num_hiddens,  # 前馈网络隐藏层维度\n",
    "        num_heads=4,              # 多头注意力的头数\n",
    "        num_layers=2,             # 编码器层数\n",
    "        dropout=0.2,              # Dropout概率\n",
    "        max_len=max_len,          # 最大序列长度，用于位置嵌入\n",
    "        key_size=256,             # 多头注意力中键的维度\n",
    "        query_size=256,           # 多头注意力中查询的维度\n",
    "        value_size=256,           # 多头注意力中值的维度\n",
    "        hid_in_features=256,      # 下一句预测任务隐藏层的输入维度\n",
    "        mlm_in_features=256,      # 掩蔽语言模型任务的输入特征维度\n",
    "        nsp_in_features=256       # 下一句预测任务的输入特征维度\n",
    "    )\n",
    "    \n",
    "    # 加载预训练参数文件（假设文件名为pretrained.params）\n",
    "    pretrained_path = os.path.join(data_dir, 'pretrained.params')\n",
    "    pretrained_dict = torch.load(pretrained_path)\n",
    "    \n",
    "    # 获取当前模型的状态字典（即模型各层的参数）\n",
    "    model_dict = bert.state_dict()\n",
    "    \n",
    "    # 过滤预训练参数字典：只保留当前模型中存在的键\n",
    "    # 这一步解决因模型结构差异导致的参数不匹配问题\n",
    "    pretrained_dict = {k: v for k, v in pretrained_dict.items() if k in model_dict}\n",
    "    \n",
    "    # 将过滤后的预训练参数合并到当前模型的状态字典中\n",
    "    model_dict.update(pretrained_dict)\n",
    "    \n",
    "    # 加载参数到模型中，strict=False表示忽略预训练中存在但当前模型没有的键\n",
    "    # 同时，模型中存在但预训练中没有的键会被随机初始化\n",
    "    bert.load_state_dict(model_dict, strict=False)\n",
    "    \n",
    "    return bert, vocab\n",
    "\n",
    "# 获取可用的计算设备（如GPU）\n",
    "devices = d2l.try_all_gpus()\n",
    "# 调用函数加载预训练模型\n",
    "bert, vocab = load_pretrained_model(\n",
    "    'bert.small', \n",
    "    num_hiddens=256, \n",
    "    ffn_num_hiddens=512, \n",
    "    num_heads=4,\n",
    "    num_layers=2, \n",
    "    dropout=0.1, \n",
    "    max_len=512, \n",
    "    devices=devices\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "942c7daa-8c81-4fd7-8718-25541ae1d6a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.utils.data as data\n",
    "import multiprocessing\n",
    "import d2l\n",
    "\n",
    "class SNLIBERTDataset(data.Dataset):\n",
    "    def __init__(self, dataset, max_len, vocab=None):\n",
    "        \"\"\"\n",
    "        初始化SNLI/BERT数据集类，用于处理文本蕴含任务的数据。\n",
    "        \n",
    "        参数:\n",
    "        dataset (list): 包含前提（premise）、假设（hypothesis）和标签的数据集。\n",
    "                        假设格式为：[前提句子列表, 假设句子列表, 标签列表]\n",
    "        max_len (int): 输入序列的最大长度，用于截断和填充。\n",
    "        vocab (Vocab, optional): 词汇表对象，用于将词元转换为索引。\n",
    "        \"\"\"\n",
    "        # 1. 对前提和假设句子进行分词处理\n",
    "        # 使用d2l.tokenize对句子列表进行分词，转为小写\n",
    "        # zip(*[...]) 将前提和假设的分词结果按顺序配对\n",
    "        all_premise_hypothesis_tokens = [[\n",
    "            p_tokens, h_tokens] for p_tokens, h_tokens in zip(\n",
    "            *[d2l.tokenize([s.lower() for s in sentences])\n",
    "              for sentences in dataset[:2]])]\n",
    "\n",
    "        # 2. 初始化标签为张量\n",
    "        self.labels = torch.tensor(dataset[2])\n",
    "        \n",
    "        # 3. 保存词汇表和最大长度\n",
    "        self.vocab = vocab\n",
    "        self.max_len = max_len\n",
    "        \n",
    "        # 4. 预处理所有前提-假设对，生成词元索引、片段索引和有效长度\n",
    "        (self.all_token_ids, self.all_segments,\n",
    "         self.valid_lens) = self._preprocess(all_premise_hypothesis_tokens)\n",
    "        \n",
    "        # 打印加载的样本数量\n",
    "        print('read ' + str(len(self.all_token_ids)) + ' examples')\n",
    "\n",
    "    def _preprocess(self, all_premise_hypothesis_tokens):\n",
    "        \"\"\"\n",
    "        多进程预处理所有前提-假设对，生成模型所需的输入格式。\n",
    "        \n",
    "        参数:\n",
    "        all_premise_hypothesis_tokens (list): 包含前提和假设词元对的列表。\n",
    "        \n",
    "        返回:\n",
    "        tuple: 包含词元索引张量、片段索引张量和有效长度张量。\n",
    "        \"\"\"\n",
    "        # 1. 创建多进程池（4个工作进程）以加速处理\n",
    "        pool = multiprocessing.Pool(4)\n",
    "        \n",
    "        # 2. 并行处理每个前提-假设对\n",
    "        # 使用pool.map将_mp_worker函数应用到所有输入对上\n",
    "        out = pool.map(self._mp_worker, all_premise_hypothesis_tokens)\n",
    "        \n",
    "        # 3. 解包处理结果\n",
    "        all_token_ids = [token_ids for token_ids, segments, valid_len in out]\n",
    "        all_segments = [segments for token_ids, segments, valid_len in out]\n",
    "        valid_lens = [valid_len for token_ids, segments, valid_len in out]\n",
    "        \n",
    "        # 4. 将结果转换为张量\n",
    "        return (torch.tensor(all_token_ids, dtype=torch.long),\n",
    "                torch.tensor(all_segments, dtype=torch.long),\n",
    "                torch.tensor(valid_lens))\n",
    "\n",
    "    def _mp_worker(self, premise_hypothesis_tokens):\n",
    "        \"\"\"\n",
    "        多进程工作函数，处理单个前提-假设对。\n",
    "        \n",
    "        参数:\n",
    "        premise_hypothesis_tokens (tuple): 包含前提和假设词元列表的元组。\n",
    "        \n",
    "        返回:\n",
    "        tuple: 处理后的词元索引、片段索引和有效长度。\n",
    "        \"\"\"\n",
    "        # 1. 解包前提和假设的词元列表\n",
    "        p_tokens, h_tokens = premise_hypothesis_tokens\n",
    "        \n",
    "        # 2. 截断词元对，确保总长度不超过max_len - 3\n",
    "        # （预留3个位置给<cls>、<sep>和<SEP>）\n",
    "        self._truncate_pair_of_tokens(p_tokens, h_tokens)\n",
    "        \n",
    "        # 3. 生成带特殊标记的词元序列和片段索引\n",
    "        tokens, segments = d2l.get_tokens_and_segments(p_tokens, h_tokens)\n",
    "        \n",
    "        # 4. 转换为词元索引并填充到max_len长度\n",
    "        # 使用词汇表将词元转换为索引，不足部分用<pad>填充\n",
    "        token_ids = self.vocab[tokens] + [self.vocab['<pad>']] * (self.max_len - len(tokens))\n",
    "        \n",
    "        # 5. 填充片段索引，不足部分用0填充\n",
    "        segments = segments + [0] * (self.max_len - len(segments))\n",
    "        \n",
    "        # 6. 记录有效长度（不包含填充部分）\n",
    "        valid_len = len(tokens)\n",
    "        \n",
    "        return token_ids, segments, valid_len\n",
    "\n",
    "    def _truncate_pair_of_tokens(self, p_tokens, h_tokens):\n",
    "        \"\"\"\n",
    "        截断前提和假设的词元列表，使其总长度不超过max_len - 3。\n",
    "        \n",
    "        参数:\n",
    "        p_tokens (list): 前提的词元列表。\n",
    "        h_tokens (list): 假设的词元列表。\n",
    "        \"\"\"\n",
    "        # 循环删除较长句子的末尾词元，直到总长度符合要求\n",
    "        while len(p_tokens) + len(h_tokens) > self.max_len - 3:\n",
    "            if len(p_tokens) > len(h_tokens):\n",
    "                p_tokens.pop()  # 删除前提的最后一个词元\n",
    "            else:\n",
    "                h_tokens.pop()  # 删除假设的最后一个词元\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        根据索引获取数据样本。\n",
    "        \n",
    "        参数:\n",
    "        idx (int): 样本索引。\n",
    "        \n",
    "        返回:\n",
    "        tuple: 包含模型输入（词元索引、片段索引、有效长度）和标签。\n",
    "        \"\"\"\n",
    "        return (self.all_token_ids[idx], self.all_segments[idx],\n",
    "                self.valid_lens[idx]), self.labels[idx]\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        获取数据集的样本数量。\n",
    "        \n",
    "        返回:\n",
    "        int: 样本数量。\n",
    "        \"\"\"\n",
    "        return len(self.all_token_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9922d060-0b87-4b00-8de4-b62b6ffb696e",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size, max_len, num_workers = 512, 128, d2l.get_dataloader_workers()\n",
    "data_dir = '..\\\\data\\\\snli_1.0' \n",
    "train_set = SNLIBERTDataset(d2l.read_snli(data_dir, True), max_len, vocab)\n",
    "test_set = SNLIBERTDataset(d2l.read_snli(data_dir, False), max_len, vocab)\n",
    "train_iter = torch.utils.data.DataLoader(train_set, batch_size, shuffle=True,\n",
    "                                   num_workers=num_workers)\n",
    "test_iter = torch.utils.data.DataLoader(test_set, batch_size,\n",
    "                                  num_workers=num_workers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b321651-73a0-42e1-b293-d67ca84cbaed",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERTClassifier(nn.Module):\n",
    "    def __init__(self, bert):\n",
    "        super(BERTClassifier, self).__init__()\n",
    "        self.encoder = bert.encoder\n",
    "        self.hidden = bert.hidden\n",
    "        self.output = nn.Linear(256, 3)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        tokens_X, segments_X, valid_lens_x = inputs\n",
    "        encoded_X = self.encoder(tokens_X, segments_X, valid_lens_x)\n",
    "        return self.output(self.hidden(encoded_X[:, 0, :]))\n",
    "\n",
    "net = BERTClassifier(bert)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d18b0326-5a1e-4a32-be62-6669646c03f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr, num_epochs = 1e-4, 5\n",
    "trainer = torch.optim.Adam(net.parameters(), lr=lr)\n",
    "loss = nn.CrossEntropyLoss(reduction='none')\n",
    "d2l.train_ch13(net, train_iter, test_iter, loss, trainer, num_epochs, devices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60629afd-71bd-47b6-b0f3-883f425dd701",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
